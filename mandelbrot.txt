To parallelize the code, I began by breaking the image into horizontal strips, assigning each node a subset of rows to compute based on its rank. This load balancing strategy ensures an even distribution of work among the nodes, minimizing idle time and maximizing overall performance. The root node broadcasts common input parameters to all nodes, ensuring that everyone has the same starting point for the computation. Once the data is distributed, each node performs the Mandelbrot set computation for its assigned rows using OpenMP for multithreading. The computation is parallelized using OpenMP directives, allowing for efficient utilization of multiple cores within each node. The code employs a dynamic scheduling strategy to balance the workload among threads, adapting to any variations in computation time across different regions of the image. After the local computation, each node holds a strip of the final image. To gather the results back to the root node, the code uses MPI_Gatherv, which efficiently collects variable-sized data from each node. The root node prepares the necessary data structures to receive the image strips from all nodes and reconstructs the complete image. To optimize file I/O, the root node writes the image to a file in binary format. 

Analyzing the results, the single node performance was drastically worse than the baseline performance. This was also the case with 2 nodes but there was a very big drop in time going from 1 to 2. By distributing the workload across two nodes, we effectively halve the computation time compared to the single-node scenario. It seems that the communication overhead introduced by MPI is outweighed by the benefits of parallel execution across multiple nodes.
4 and 8 nodes achieved very fast times, beating out the serial implementation. However, the decrease in time is significantly less than from 1-2 nodes, indicating that the overhead of MPI and other bottlenecks are limiting the parallel regions.  